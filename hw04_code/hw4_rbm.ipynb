{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC 636, HW4: Restricted Boltzman Machine on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset with labels encoded as one-hot vectors\n",
    "class Dataset():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "        self.epochs = 0\n",
    "\n",
    "    def shuffle(self):\n",
    "        perm = np.arange(self.data[0].shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        self.data = tuple(datai[perm] for datai in self.data)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.index\n",
    "        end = self.index + batch_size\n",
    "        if end > self.data[0].shape[0]:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.index, start = 0, 0\n",
    "            end = batch_size\n",
    "        self.index = end\n",
    "        return tuple(datai[start:end, ...] for datai in self.data)\n",
    "            \n",
    "def load_mnist():\n",
    "    def preprocess(data, labels, num_classes):\n",
    "        # flatten images\n",
    "        data = data.astype(np.float32)/255.0\n",
    "        data = np.reshape(data, [data.shape[0], -1])\n",
    "        # one hot encoding\n",
    "        num_labels = labels.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels.ravel()] = 1\n",
    "        return data, labels_one_hot\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train = preprocess(train[0], train[1], 10)\n",
    "    test = preprocess(test[0], test[1], 10)\n",
    "    return SimpleNamespace(\n",
    "        train=Dataset(train), \n",
    "        test=Dataset(test))\n",
    "mnist = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_sample_tf(x):\n",
    "    ''' sample from bernoulli distribution given a tf matrix '''\n",
    "    in_shape= x.get_shape().as_list()\n",
    "    uniform_samp = tf.random_uniform(shape=(in_shape[0], in_shape[1]), minval=0.0, maxval=1.0) \n",
    "    return tf.to_float(tf.greater(x, uniform_samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the contrastive divergence update rule for the rbm, using a mini-batch of  ğ‘€  samples:\n",
    "\n",
    "ğ‘Šâ†ğ‘Š+ğ›¼Î”ğ‘Š  \n",
    "ğ‘â†ğ‘+ğ›¼Î”ğ‘  \n",
    "ğ‘â†ğ‘+ğ›¼Î”ğ‘  \n",
    "\n",
    "Where:\n",
    "\n",
    "ğ‘Šâˆˆîˆ¾î‰‚Ã—îˆ´,bâˆˆîˆ¾îˆ´,câˆˆîˆ¾î‰‚  \n",
    "\n",
    "ğ‘‹âˆˆîˆ¾îˆ¹Ã—î‰‚  , is the matrix composed by the training samples in the mini-batch. Each row of  ğ‘‹  is a training sample \n",
    "\n",
    "Î”ğ‘Š=1ğ‘€(ğ‘‹ğ‘‡â„(ğ‘¥)âˆ’ğ‘‹ğ‘‡ğ‘ â„(ğ‘‹ğ‘ ))  \n",
    "\n",
    "Î”ğ‘=ğ‘šğ‘’ğ‘ğ‘›((â„(ğ‘‹)âˆ’â„(ğ‘‹ğ‘ )))  \n",
    "\n",
    "Î”ğ‘=ğ‘šğ‘’ğ‘ğ‘›((ğ‘‹âˆ’ğ‘‹ğ‘ ))  \n",
    "\n",
    "â„(ğ‘‹)=ğœ(ğ‘‹ğ‘Š+ğ‘);ğœ(ğ‘¥)=11+exp(âˆ’ğ‘¥) \n",
    "Note: The model uses Bernoulli visible and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-0eaf666a8b05>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-0eaf666a8b05>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    n_inputs=       # Complete: number of input units (i.e. V)\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_inputs=       # Complete: number of input units (i.e. V)\n",
    "n_hidden=       # Complete: number of hidden units (i.e. H), try different values\n",
    "batch_size= 500 # number of samples on the mini-batch (i.e. M)\n",
    "\n",
    "k= 3          #Try different values \n",
    "alpha= 0.1   #Try different values\n",
    "\n",
    "X = tf.placeholder( tf.float32, shape=(batch_size, n_inputs))\n",
    "\n",
    "W= # Complete: weight matrix for the rbm model\n",
    "b= # Complete: b vector for the rbm model\n",
    "c= # Complete: c vector for the rbm model\n",
    "\n",
    "# K gibbs sampling: obtain a set of samples Xs by performing K-gibbs sampling steps\n",
    "Xs = X;\n",
    "for i in range(k):\n",
    "    hs_prob = tf.sigmoid(tf.matmul(Xs, W) + b)\n",
    "    hs = bernoulli_sample_tf(hs_prob)\n",
    "    Xs_prob = tf.sigmoid(tf.matmul(hs, tf.transpose(W)) + c)\n",
    "    Xs = bernoulli_sample_tf(Xs_prob)\n",
    "    \n",
    "# Parameter update:\n",
    "h_x =   # Complete: compute h(X)\n",
    "h_xs =  # Complete: compute h(Xs)\n",
    "\n",
    "dW =    # Complete: follow update equations\n",
    "     \n",
    "        \n",
    "db =    # Complete: follow update equations\n",
    "dc =    # Complete: follow update equations\n",
    "\n",
    "op_W= W.assign_add( alpha*dW )\n",
    "op_b= b.assign_add( alpha*db )\n",
    "op_c= c.assign_add( alpha*dc )\n",
    "\n",
    "update_weights = tf.group( op_W, op_b, op_c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cea2ab1bd015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For visualization of the samples from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# K gibbs sampling starting from random hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mXst_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# For visualization of the samples from the model\n",
    "# K gibbs sampling starting from random hidden layer\n",
    "hst = tf.random_uniform(shape=(batch_size, n_hidden), minval=0, maxval=1)\n",
    "for i in range(10):\n",
    "    Xst_prob = tf.sigmoid(tf.matmul(hst, tf.transpose(W)) + c)\n",
    "    Xst = bernoulli_sample_tf(Xst_prob)\n",
    "    hst_prob = tf.sigmoid(tf.matmul(Xst, W) + b)\n",
    "    hst = bernoulli_sample_tf(hst_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print('Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps= 10000 # try different values\n",
    "n_logging = 500  # try different values\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # -------- train RBM -----#\n",
    "    batch_x, _= mnist.train.next_batch(batch_size) \n",
    "    [_, x_g] = sess.run([update_weights, Xs_prob], feed_dict= {X : batch_x})\n",
    "    \n",
    "    #  ------- logging -------\n",
    "    if step%n_logging == 0:\n",
    "        clear_output()\n",
    "        \n",
    "        # --- plot samples starting from given x --- #\n",
    "        x_gaux = np.reshape(x_g, [-1,28,28,1])\n",
    "        \n",
    "        plt.figure(1)\n",
    "        print('Samples generated starting from given X')\n",
    "        for i in range(2*4):\n",
    "            plt.subplot(241 + i)\n",
    "            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n",
    "        plt.show()\n",
    "        \n",
    "        # --- plot random samples --- #\n",
    "        [x_g] = sess.run([Xst_prob], feed_dict= {X : batch_x})\n",
    "        x_gaux = np.reshape(x_g, [-1,28,28,1])\n",
    "        \n",
    "        plt.figure(2)\n",
    "        print('Samples generated from random H')\n",
    "        for i in range(2*4):\n",
    "            plt.subplot(241 + i)\n",
    "            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
