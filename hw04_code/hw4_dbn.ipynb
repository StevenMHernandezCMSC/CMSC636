{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC 636, HW4: Deep Belief Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset with labels encoded as one-hot vectors\n",
    "class Dataset():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "        self.epochs = 0\n",
    "\n",
    "    def shuffle(self):\n",
    "        perm = np.arange(self.data[0].shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        self.data = tuple(datai[perm] for datai in self.data)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.index\n",
    "        end = self.index + batch_size\n",
    "        if end > self.data[0].shape[0]:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.index, start = 0, 0\n",
    "            end = batch_size\n",
    "        self.index = end\n",
    "        return tuple(datai[start:end, ...] for datai in self.data)\n",
    "            \n",
    "def load_mnist():\n",
    "    def preprocess(data, labels, num_classes):\n",
    "        # flatten images\n",
    "        data = data.astype(np.float32)/255.0\n",
    "        data = np.reshape(data, [data.shape[0], -1])\n",
    "        # one hot encoding\n",
    "        num_labels = labels.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels.ravel()] = 1\n",
    "        return data, labels_one_hot\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train = preprocess(train[0], train[1], 10)\n",
    "    test = preprocess(test[0], test[1], 10)\n",
    "    return SimpleNamespace(\n",
    "        train=Dataset(train), \n",
    "        test=Dataset(test))\n",
    "mnist = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_sample_tf(x):\n",
    "    ''' sample from bernoulli distribution given a tf matrix '''\n",
    "    in_shape= x.get_shape().as_list()\n",
    "    uniform_samp = tf.random_uniform(shape=(in_shape[0], in_shape[1]), minval=0.0, maxval=1.0) \n",
    "    return tf.to_float(tf.greater(x, uniform_samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the contrastive divergence update rule for the rbm, using a mini-batch of  ğ‘€  samples:\n",
    "\n",
    "ğ‘Šâ†ğ‘Š+ğ›¼Î”ğ‘Š  \n",
    "ğ‘â†ğ‘+ğ›¼Î”ğ‘  \n",
    "ğ‘â†ğ‘+ğ›¼Î”ğ‘  \n",
    "\n",
    "Where:\n",
    "\n",
    "ğ‘Šâˆˆîˆ¾î‰‚Ã—îˆ´,bâˆˆîˆ¾îˆ´,câˆˆîˆ¾î‰‚  \n",
    "\n",
    "ğ‘‹âˆˆîˆ¾îˆ¹Ã—î‰‚  , is the matrix composed by the training samples in the mini-batch. Each row of  ğ‘‹  is a training sample \n",
    "\n",
    "Î”ğ‘Š=1ğ‘€(ğ‘‹ğ‘‡â„(ğ‘¥)âˆ’ğ‘‹ğ‘‡ğ‘ â„(ğ‘‹ğ‘ ))  \n",
    "\n",
    "Î”ğ‘=ğ‘šğ‘’ğ‘ğ‘›((â„(ğ‘‹)âˆ’â„(ğ‘‹ğ‘ )))  \n",
    "\n",
    "Î”ğ‘=ğ‘šğ‘’ğ‘ğ‘›((ğ‘‹âˆ’ğ‘‹ğ‘ ))  \n",
    "\n",
    "â„(ğ‘‹)=ğœ(ğ‘‹ğ‘Š+ğ‘);ğœ(ğ‘¥)=11+exp(âˆ’ğ‘¥) \n",
    "Note: The model uses Bernoulli visible and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-7aeaa3def192>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-7aeaa3def192>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    h_x =   # Complete: h(X)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Complete the following function using the code from the hw4_rbm script \n",
    "def rbm_model(X, W, b, c, k, alpha):\n",
    "    # K gibbs sampling\n",
    "    Xs = X;\n",
    "    for i in range(k):\n",
    "        hs_prob = tf.sigmoid(tf.matmul(Xs, W) + b)\n",
    "        hs = bernoulli_sample_tf(hs_prob)\n",
    "        Xs_prob = tf.sigmoid(tf.matmul(hs, tf.transpose(W)) + c)\n",
    "        Xs = bernoulli_sample_tf(Xs_prob)\n",
    "\n",
    "    # Parameter update:\n",
    "    h_x =   # Complete: h(X)\n",
    "    h_xs =  # Complete: h(Xs)\n",
    "\n",
    "    dW =  # Complete: follow update equations\n",
    "         \n",
    "\n",
    "    db =  # Complete: follow update equations\n",
    "    dc =  # Complete: follow update equations\n",
    "\n",
    "    op_W= W.assign_add( alpha*dW )\n",
    "    op_b= b.assign_add( alpha*db )\n",
    "    op_c= c.assign_add( alpha*dc )\n",
    "\n",
    "    update_weights = tf.group( op_W, op_b, op_c )\n",
    "    \n",
    "    return update_weights, Xs_prob, h_x, h_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-5a5ca3546347>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-5a5ca3546347>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    n_inputs=               # Complete: number of input units (i.e. V)\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_inputs=               # Complete: number of input units (i.e. V)\n",
    "n_hidden= [ 100, 20]    # number of hidden units (i.e. H), try different values\n",
    "batch_size= 500         # number of samples on the mini-batch (i.e. M)\n",
    "\n",
    "k= 5       #Try different values\n",
    "\n",
    "X = tf.placeholder( tf.float32, shape=(batch_size, n_inputs))\n",
    "\n",
    "# Definition of first RBM model\n",
    "W1= # Complete: weight matrix for the rbm model on the first layer\n",
    "b1= # Complete: b vector for the rbm model on the first layer\n",
    "c1= # Complete: c vector for the rbm model on the first layer\n",
    "\n",
    "alpha= 0.1  #Try different values\n",
    "update_weights1, Xs_prob, h1_x, h1_xs = rbm_model( X, W1, b1, c1, k, alpha )\n",
    "\n",
    "# Definition of second RBM model \n",
    "W2= # Complete: weight matrix for the rbm model of the second layer\n",
    "b2= # Complete: b vector for the rbm model on the second layer\n",
    "c2= # Complete: c vector for the rbm model on the second layer\n",
    "\n",
    "alpha= 0.1 #Try different values\n",
    "update_weights2, h1s_prob, h2_h1, h2_h1s = rbm_model( h1_x, W2, b2, c2, k, alpha )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h1s_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5f30384e07c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXs_prob_h2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1s_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# samples given X while training second RBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# random samples from DBN model starting with random h1 samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mh1st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h1s_prob' is not defined"
     ]
    }
   ],
   "source": [
    "Xs_prob_h2 = tf.sigmoid(tf.matmul(h1s_prob, tf.transpose(W1)) + c1) # samples given X while training second RBM\n",
    "\n",
    "# random samples from DBN model starting with random h1 samples\n",
    "h1st = tf.random_uniform(shape=(batch_size, n_hidden[0]), minval=0, maxval=1)\n",
    "for i in range(10):\n",
    "    Xst_prob = tf.sigmoid(tf.matmul(h1st, tf.transpose(W1)) + c1)\n",
    "    Xst = bernoulli_sample_tf(Xst_prob)\n",
    "    h1st_prob = tf.sigmoid(tf.matmul(Xst, W1) + b1)\n",
    "    h1st = bernoulli_sample_tf(h1st_prob)\n",
    "Xst_prob_h1st = tf.sigmoid(tf.matmul(h1st, tf.transpose(W1)) + c1) # X generated from random h1\n",
    "\n",
    "\n",
    "# random samples from DBN model starting with random h2 samples\n",
    "h2st = tf.random_uniform(shape=(batch_size, n_hidden[1]), minval=0, maxval=1)\n",
    "for i in range(10):\n",
    "    h1st_prob = tf.sigmoid(tf.matmul(h2st, tf.transpose(W2)) + c2)\n",
    "    h1st = bernoulli_sample_tf(h1st_prob)\n",
    "    h2st_prob = tf.sigmoid(tf.matmul(h1st, W2) + b2)\n",
    "    h2st = bernoulli_sample_tf(h2st_prob)\n",
    "Xst_prob_h2st = tf.sigmoid(tf.matmul(h1st, tf.transpose(W1)) + c1) # X generated from random h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print('Initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of layer 1 (first RBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-16c2508264a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# -------- train RBM -----#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_g\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_weights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs_prob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps= 10000 # try different values\n",
    "n_logging = 500  # try different values\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # -------- train RBM -----#\n",
    "    batch_x, _= mnist.train.next_batch(batch_size) \n",
    "    [_, x_g] = sess.run([update_weights1, Xs_prob], feed_dict= {X : batch_x})\n",
    "    \n",
    "    #  ------- logging -------\n",
    "    if step%n_logging == 0:\n",
    "        clear_output()\n",
    "        print('Samples generated starting from given X')\n",
    "        \n",
    "        x_gaux = np.reshape(x_g, [-1,28,28,1])        \n",
    "        plt.figure(1)\n",
    "        for i in range(2*4):\n",
    "            plt.subplot(241 + i)\n",
    "            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Samples generated from random H1')\n",
    "        [x_g] = sess.run([Xst_prob_h1st], feed_dict= {X : batch_x})\n",
    "        x_gaux = np.reshape(x_g, [-1,28,28,1])\n",
    "        \n",
    "        plt.figure(2)\n",
    "        for i in range(2*4):\n",
    "            plt.subplot(241 + i)\n",
    "            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of layer 2 (second RBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f1eca774f415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# -------- train RBM -----#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_g\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_weights2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs_prob_h2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps= 10000 # try different values\n",
    "n_logging = 500  # try different values\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # -------- train RBM -----#\n",
    "    batch_x, _= mnist.train.next_batch(batch_size) \n",
    "    [_, x_g] = sess.run([update_weights2, Xs_prob_h2], feed_dict= {X : batch_x})\n",
    "    \n",
    "    #  ------- logging -------\n",
    "    if step%n_logging == 0:\n",
    "        clear_output()\n",
    "        print('Samples generated starting from given X')\n",
    "        x_gaux = np.reshape(x_g, [-1,28,28,1])\n",
    "        \n",
    "        plt.figure(1)\n",
    "        for i in range(2*4):\n",
    "            plt.subplot(241 + i)\n",
    "            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Samples generated from random H2')\n",
    "        [x_g] = sess.run([Xst_prob_h2st], feed_dict= {X : batch_x})\n",
    "        x_gaux = np.reshape(x_g, [-1,28,28,1])\n",
    "        \n",
    "        plt.figure(2)\n",
    "        for i in range(2*4):\n",
    "            plt.subplot(241 + i)\n",
    "            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
